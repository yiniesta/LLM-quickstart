{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_D9kG_efts3"
   },
   "source": [
    "# Transformers 模型量化技术：AWQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WE9IhcVyktah"
   },
   "source": [
    "![img](https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/Thumbnail.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Wwsg6nCwoThm"
   },
   "source": [
    "在2023年6月，Ji Lin等人发表了论文[AWQ：Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/pdf/2306.00978.pdf)。\n",
    "\n",
    "这篇论文详细介绍了一种激活感知权重量化算法，可以用于压缩任何基于 Transformer 的语言模型，同时只有微小的性能下降。关于 AWQ 算法的详细介绍，见[MIT Han Song 教授分享](https://hanlab.mit.edu/projects/awq)。\n",
    "\n",
    "transformers 现在支持两个不同的 AWQ 开源实现库：\n",
    "\n",
    "- [AutoAWQ](https://github.com/casper-hansen/AutoAWQ)\n",
    "- [LLM-AWQ](https://github.com/mit-han-lab/llm-awq) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H2019RkoiM-"
   },
   "source": [
    "因为 LLM-AWQ 不支持 Nvidia T4 GPU（课程演示 GPU），所以我们使用 AutoAWQ 库来介绍和演示 AWQ 模型量化技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 量化前模型测试文本生成任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_path = \"facebook/opt-125m\"\n",
    "\n",
    "# 使用 GPU 加载原始的 OPT-125m 模型\n",
    "generator = pipeline('text-generation',\n",
    "                     model=model_path,\n",
    "                     device=0,\n",
    "                     do_sample=True,\n",
    "                     num_return_sequences=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实测GPU显存占用：加载 OPT-125m 模型后\n",
    "\n",
    "```shell\n",
    "Sun Dec 24 15:11:33 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   47C    P0              26W /  70W |    635MiB / 15360MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The woman worked as a school nurse and the man was a teacher. The men were the teachers!'},\n",
       " {'generated_text': 'The woman worked as a waitress, waitress, barista and food truck driver, and was an early'},\n",
       " {'generated_text': 'The woman worked as a security guard at a motel in her hometown in San Francisco. Photograph: T'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"The woman worked as a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The man worked as a delivery boy before. He made a fortune as a courier and it makes no'},\n",
       " {'generated_text': 'The man worked as a taxi driver for five years. He had a lot to do, and there'},\n",
       " {'generated_text': 'The man worked as a security guard at a gas station at the time and was a co-worker'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"The man worked as a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dJJRQ2p7eLQ"
   },
   "source": [
    "## 使用 AutoAWQ 量化模型\n",
    "\n",
    "下面我们以 `facebook opt-125m` 模型为例，使用 `AutoAWQ` 库实现的 AWQ 算法实现模型量化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23758a457e242c4940c875840177e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "quant_path = \"models/opt-125m-awq\"\n",
    "quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\"}\n",
    "\n",
    "# 加载模型\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Qn_P_E5p7gAN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.11/site-packages/huggingface_hub-0.19.4-py3.8.egg/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "AWQ: 100%|██████████| 12/12 [01:19<00:00,  6.59s/it]\n"
     ]
    }
   ],
   "source": [
    "# 量化模型\n",
    "model.quantize(tokenizer, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实测GPU显存使用：量化模型时峰值达到将近 4GB\n",
    "\n",
    "```shell\n",
    "Sun Dec 24 15:12:50 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   48C    P0              32W /  70W |    3703MiB / 15360MiB |      2%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nVzKDBlP_6MV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zero_point': True, 'q_group_size': 128, 'w_bit': 4, 'version': 'GEMM'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuPLq9sa8EaN"
   },
   "source": [
    "#### Transformers 兼容性配置\n",
    "\n",
    "为了使`quant_config` 与 transformers 兼容，我们需要修改配置文件：`使用 Transformers.AwqConfig 来实例化量化模型配置`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KE8xjwlL8DnA"
   },
   "outputs": [],
   "source": [
    "from transformers import AwqConfig, AutoConfig\n",
    "\n",
    "# 修改配置文件以使其与transformers集成兼容\n",
    "quantization_config = AwqConfig(\n",
    "    bits=quant_config[\"w_bit\"],\n",
    "    group_size=quant_config[\"q_group_size\"],\n",
    "    zero_point=quant_config[\"zero_point\"],\n",
    "    version=quant_config[\"version\"].lower(),\n",
    ").to_dict()\n",
    "\n",
    "# 预训练的transformers模型存储在model属性中，我们需要传递一个字典\n",
    "model.model.config.quantization_config = quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:`quant_config.json` is being deprecated in the future in favor of quantization_config in config.json.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/opt-125m-awq/tokenizer_config.json',\n",
       " 'models/opt-125m-awq/special_tokens_map.json',\n",
       " 'models/opt-125m-awq/vocab.json',\n",
       " 'models/opt-125m-awq/merges.txt',\n",
       " 'models/opt-125m-awq/added_tokens.json',\n",
       " 'models/opt-125m-awq/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型权重\n",
    "model.save_quantized(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)  # 保存分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 GPU 加载量化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(quant_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(quant_path, device_map=\"cuda\").to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "    out = model.generate(**inputs, max_new_tokens=64)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merry Christmas! I'm glad to hear you're doing well!\n",
      "Thank you! I'm glad to hear you're doing well!\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"Merry Christmas! I'm glad to\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Z0hAXYanCDW3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The woman worked as a nurse at the hospital for a year. She was a nurse at the hospital for a year. She was a nurse at the hospital for a year. She was a nurse at the hospital for a year. She was a nurse at the hospital for a year. She was a nurse at the hospital for a year. She\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"The woman worked as a\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework：使用 AWQ 量化 Facebook OPT-6.7B 模型\n",
    "\n",
    "Facebook OPT 模型：https://huggingface.co/facebook?search_models=opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 服务器GPU内存不足，使用facebook/opt-1.3b跑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-12-29 12:39:38.462454: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-29 12:39:38.462514: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-29 12:39:38.462551: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-29 12:39:38.476430: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-29 12:39:39.780742: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"facebook/opt-1.3b\"\n",
    "quant_path = \"models/opt-1.3b-awq\"\n",
    "quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "LICENSE.md: 100%|██████████| 11.1k/11.1k [00:00<00:00, 21.8MB/s]\n",
      "\n",
      "\n",
      ".gitattributes:   0%|          | 0.00/1.17k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "README.md: 100%|██████████| 8.82k/8.82k [00:00<00:00, 38.5MB/s]\n",
      ".gitattributes: 100%|██████████| 1.17k/1.17k [00:00<00:00, 596kB/s]\n",
      "Fetching 10 files: 100%|██████████| 10/10 [00:00<00:00, 44.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 167/167 [00:00<00:00, 1.23MB/s]\n",
      "/root/miniconda3/lib/python3.11/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n",
      "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/471M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   1%|          | 4.19M/471M [00:00<00:20, 22.6MB/s]\u001b[A\n",
      "Downloading data:   3%|▎         | 12.6M/471M [00:00<00:20, 22.8MB/s]\u001b[A\n",
      "Downloading data:   4%|▍         | 21.0M/471M [00:01<00:28, 15.9MB/s]\u001b[A\n",
      "Downloading data:   6%|▌         | 29.4M/471M [00:01<00:30, 14.4MB/s]\u001b[A\n",
      "Downloading data:   8%|▊         | 37.7M/471M [00:02<00:31, 13.6MB/s]\u001b[A\n",
      "Downloading data:  10%|▉         | 46.1M/471M [00:03<00:32, 13.2MB/s]\u001b[A\n",
      "Downloading data:  12%|█▏        | 54.5M/471M [00:03<00:32, 13.0MB/s]\u001b[A\n",
      "Downloading data:  13%|█▎        | 62.9M/471M [00:04<00:31, 12.8MB/s]\u001b[A\n",
      "Downloading data:  15%|█▌        | 71.3M/471M [00:05<00:31, 12.7MB/s]\u001b[A\n",
      "Downloading data:  17%|█▋        | 79.7M/471M [00:05<00:30, 12.6MB/s]\u001b[A\n",
      "Downloading data:  19%|█▊        | 88.1M/471M [00:06<00:33, 11.5MB/s]\u001b[A\n",
      "Downloading data:  20%|██        | 96.5M/471M [00:07<00:28, 13.0MB/s]\u001b[A\n",
      "Downloading data:  22%|██▏       | 105M/471M [00:07<00:28, 12.8MB/s] \u001b[A\n",
      "Downloading data:  24%|██▍       | 113M/471M [00:08<00:28, 12.7MB/s]\u001b[A\n",
      "Downloading data:  26%|██▌       | 122M/471M [00:09<00:27, 12.6MB/s]\u001b[A\n",
      "Downloading data:  28%|██▊       | 130M/471M [00:09<00:27, 12.6MB/s]\u001b[A\n",
      "Downloading data:  29%|██▉       | 138M/471M [00:10<00:26, 12.6MB/s]\u001b[A\n",
      "Downloading data:  31%|███       | 147M/471M [00:11<00:25, 12.5MB/s]\u001b[A\n",
      "Downloading data:  33%|███▎      | 155M/471M [00:11<00:25, 12.6MB/s]\u001b[A\n",
      "Downloading data:  35%|███▍      | 164M/471M [00:12<00:24, 12.5MB/s]\u001b[A\n",
      "Downloading data:  37%|███▋      | 172M/471M [00:13<00:23, 12.5MB/s]\u001b[A\n",
      "Downloading data:  38%|███▊      | 180M/471M [00:13<00:23, 12.5MB/s]\u001b[A\n",
      "Downloading data:  40%|████      | 189M/471M [00:14<00:22, 12.5MB/s]\u001b[A\n",
      "Downloading data:  42%|████▏     | 197M/471M [00:15<00:21, 12.5MB/s]\u001b[A\n",
      "Downloading data:  44%|████▎     | 206M/471M [00:15<00:21, 12.5MB/s]\u001b[A\n",
      "Downloading data:  45%|████▌     | 214M/471M [00:16<00:20, 12.5MB/s]\u001b[A\n",
      "Downloading data:  47%|████▋     | 222M/471M [00:17<00:19, 12.5MB/s]\u001b[A\n",
      "Downloading data:  49%|████▉     | 231M/471M [00:17<00:19, 12.5MB/s]\u001b[A\n",
      "Downloading data:  51%|█████     | 239M/471M [00:18<00:18, 12.5MB/s]\u001b[A\n",
      "Downloading data:  53%|█████▎    | 247M/471M [00:19<00:19, 11.5MB/s]\u001b[A\n",
      "Downloading data:  54%|█████▍    | 256M/471M [00:20<00:16, 12.9MB/s]\u001b[A\n",
      "Downloading data:  56%|█████▌    | 264M/471M [00:20<00:17, 12.1MB/s]\u001b[A\n",
      "Downloading data:  58%|█████▊    | 273M/471M [00:21<00:15, 12.9MB/s]\u001b[A\n",
      "Downloading data:  60%|█████▉    | 281M/471M [00:22<00:14, 12.8MB/s]\u001b[A\n",
      "Downloading data:  61%|██████▏   | 289M/471M [00:22<00:14, 12.7MB/s]\u001b[A\n",
      "Downloading data:  63%|██████▎   | 298M/471M [00:23<00:13, 12.7MB/s]\u001b[A\n",
      "Downloading data:  65%|██████▌   | 306M/471M [00:24<00:13, 12.6MB/s]\u001b[A\n",
      "Downloading data:  67%|██████▋   | 315M/471M [00:24<00:12, 12.6MB/s]\u001b[A\n",
      "Downloading data:  69%|██████▊   | 323M/471M [00:25<00:11, 12.6MB/s]\u001b[A\n",
      "Downloading data:  70%|███████   | 331M/471M [00:26<00:11, 12.5MB/s]\u001b[A\n",
      "Downloading data:  72%|███████▏  | 340M/471M [00:26<00:10, 12.5MB/s]\u001b[A\n",
      "Downloading data:  74%|███████▍  | 348M/471M [00:27<00:09, 12.5MB/s]\u001b[A\n",
      "Downloading data:  76%|███████▌  | 357M/471M [00:28<00:09, 12.5MB/s]\u001b[A\n",
      "Downloading data:  77%|███████▋  | 365M/471M [00:28<00:08, 12.5MB/s]\u001b[A\n",
      "Downloading data:  79%|███████▉  | 373M/471M [00:29<00:07, 12.5MB/s]\u001b[A\n",
      "Downloading data:  81%|████████  | 382M/471M [00:30<00:07, 12.5MB/s]\u001b[A\n",
      "Downloading data:  83%|████████▎ | 390M/471M [00:30<00:06, 12.5MB/s]\u001b[A\n",
      "Downloading data:  85%|████████▍ | 398M/471M [00:31<00:05, 12.3MB/s]\u001b[A\n",
      "Downloading data:  86%|████████▋ | 407M/471M [00:32<00:05, 12.6MB/s]\u001b[A\n",
      "Downloading data:  88%|████████▊ | 415M/471M [00:32<00:04, 12.6MB/s]\u001b[A\n",
      "Downloading data:  90%|████████▉ | 424M/471M [00:33<00:03, 12.5MB/s]\u001b[A\n",
      "Downloading data:  92%|█████████▏| 432M/471M [00:34<00:03, 12.5MB/s]\u001b[A\n",
      "Downloading data:  94%|█████████▎| 440M/471M [00:34<00:02, 12.5MB/s]\u001b[A\n",
      "Downloading data:  95%|█████████▌| 449M/471M [00:35<00:01, 12.5MB/s]\u001b[A\n",
      "Downloading data:  97%|█████████▋| 457M/471M [00:36<00:01, 12.5MB/s]\u001b[A\n",
      "Downloading data: 100%|██████████| 471M/471M [00:36<00:00, 12.7MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 1/1 [00:36<00:00, 37.00s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:03<00:00,  3.59s/it]\n",
      "Generating validation split: 214670 examples [00:05, 39337.17 examples/s]\n",
      "AWQ: 100%|██████████| 24/24 [09:51<00:00, 24.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# 量化模型\n",
    "model.quantize(tokenizer, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zero_point': True, 'q_group_size': 128, 'w_bit': 4, 'version': 'GEMM'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AwqConfig, AutoConfig\n",
    "\n",
    "# 修改配置文件以使其与transformers集成兼容\n",
    "quantization_config = AwqConfig(\n",
    "    bits=quant_config[\"w_bit\"],\n",
    "    group_size=quant_config[\"q_group_size\"],\n",
    "    zero_point=quant_config[\"zero_point\"],\n",
    "    version=quant_config[\"version\"].lower(),\n",
    ").to_dict()\n",
    "\n",
    "# 预训练的transformers模型存储在model属性中，我们需要传递一个字典\n",
    "model.model.config.quantization_config = quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:`quant_config.json` is being deprecated in the future in favor of quantization_config in config.json.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/opt-1.3b-awq/tokenizer_config.json',\n",
       " 'models/opt-1.3b-awq/special_tokens_map.json',\n",
       " 'models/opt-1.3b-awq/vocab.json',\n",
       " 'models/opt-1.3b-awq/merges.txt',\n",
       " 'models/opt-1.3b-awq/added_tokens.json',\n",
       " 'models/opt-1.3b-awq/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型权重\n",
    "model.save_quantized(quant_path)\n",
    "# 保存分词器\n",
    "tokenizer.save_pretrained(quant_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptAWQForCausalLM(\n",
       "  (model): OPTForCausalLM(\n",
       "    (model): OPTModel(\n",
       "      (decoder): OPTDecoder(\n",
       "        (embed_tokens): Embedding(50272, 2048, padding_idx=1)\n",
       "        (embed_positions): OPTLearnedPositionalEmbedding(2050, 2048)\n",
       "        (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x OPTDecoderLayer(\n",
       "            (self_attn): OPTAttention(\n",
       "              (k_proj): WQLinear_GEMM(in_features=2048, out_features=2048, bias=True, w_bit=4, group_size=128)\n",
       "              (v_proj): WQLinear_GEMM(in_features=2048, out_features=2048, bias=True, w_bit=4, group_size=128)\n",
       "              (q_proj): WQLinear_GEMM(in_features=2048, out_features=2048, bias=True, w_bit=4, group_size=128)\n",
       "              (out_proj): WQLinear_GEMM(in_features=2048, out_features=2048, bias=True, w_bit=4, group_size=128)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): WQLinear_GEMM(in_features=2048, out_features=8192, bias=True, w_bit=4, group_size=128)\n",
       "            (fc2): WQLinear_GEMM(in_features=8192, out_features=2048, bias=True, w_bit=4, group_size=128)\n",
       "            (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=50272, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(quant_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(quant_path, device_map=\"cuda\").to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "    out = model.generate(**inputs, max_new_tokens=64)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The woman worked as a nurse in the hospital where the man worked.\n",
      "\n",
      "The man was arrested on suspicion of assault and was taken to hospital for treatment.\n",
      "\n",
      "The woman was taken to hospital for treatment.\n",
      "\n",
      "A man has been arrested on suspicion of assault after a woman was injured in a hospital.\n",
      "\n",
      "The incident happened at\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"The woman worked as a\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merry Christmas! I'm glad to see you're still around.\n",
      "I'm glad to be here!\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"Merry Christmas! I'm glad to\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新年好啊，很高兴\n",
      "\n",
      "第1章 公式サイト\n",
      "\n",
      "第2章 公式サイト\n",
      "\n",
      "第3章 公式サイト\n",
      "\n",
      "第4章 公式サイト\n",
      "\n",
      "�\n"
     ]
    }
   ],
   "source": [
    "result = generate_text(\"新年好啊，很高兴\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
